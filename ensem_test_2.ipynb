{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this file only test 2 models together, and train_moe_labels.txt contains numbers from\n",
    "# all 5 models, we gotta force it all into 2 numbers only. (Or else cuda will crash xpp)\n",
    "\n",
    "with open('train.zh-en.zh', 'r') as train_moe_text_file, \\\n",
    "    open('train_moe_labels.txt','r') as train_moe_labels_file, \\\n",
    "    open('filtered_train_moe_text.txt', 'w') as filtered_train_moe_text_file, \\\n",
    "    open('filtered_train_moe_labels.txt', 'w') as filtered_train_moe_labels_file:\n",
    "    \n",
    "    for text_line, best_idx in zip(train_moe_text_file, train_moe_labels_file):\n",
    "        best_idx = int(best_idx)\n",
    "        if not (best_idx == 0 or best_idx == 3):\n",
    "            continue\n",
    "\n",
    "        filtered_train_moe_text_file.write(text_line)\n",
    "        # small_100\n",
    "        if best_idx == 3:\n",
    "            filtered_train_moe_labels_file.write(str(1) + '\\n')\n",
    "        # mariant\n",
    "        else:\n",
    "            filtered_train_moe_labels_file.write(str(0) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\cs4248\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\User\\.conda\\envs\\cs4248\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, M2M100ForConditionalGeneration, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "models = [\n",
    "    AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\").to(\"cuda\"),\n",
    "    M2M100ForConditionalGeneration.from_pretrained(\"alirezamsh/small100\").to(\"cuda\")\n",
    "]\n",
    "\n",
    "tokenizers = [\n",
    "    AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\"),\n",
    "    AutoTokenizer.from_pretrained(\"alirezamsh/small100\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = torch.nn.Linear(193113, 1024).to('cuda')\n",
    "        self.l2 = torch.nn.LeakyReLU(0.1)\n",
    "        self.l3 = torch.nn.Dropout(0.2)\n",
    "        self.l4 = torch.nn.Linear(1024, 128).to('cuda')\n",
    "        self.l5 = torch.nn.LeakyReLU(0.1)\n",
    "        self.l6 = torch.nn.Dropout(0.2)\n",
    "        self.l7 = torch.nn.Linear(128, 2).to('cuda')\n",
    "\n",
    "    def forward(self, concatted_outputs):\n",
    "        # print(len(concatted_outputs))\n",
    "        x = self.l1(concatted_outputs)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.l6(x)\n",
    "        x = self.l7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_file\n",
    "from torch.utils.data import Dataset as Ds, Subset\n",
    "\n",
    "class TrainingDataset(Ds):\n",
    "    def __init__(self, text_path, lab_path, models, model_tokenizers):\n",
    "        '''\n",
    "        dataset_reduce_scale = reduce the sample size of the dataset. \n",
    "        E.g dataset_reduce_scale=5 on sample size 100, basically reduce sample size from 100 to 20.\n",
    "        '''\n",
    "        self.untranslated_texts = read_file(text_path)\n",
    "        self.best_model_idx_labels = read_file(lab_path)\n",
    "\n",
    "        self.model_tokenizers = model_tokenizers\n",
    "        self.models = models\n",
    "\n",
    "        start_token_ids = [model.config.decoder_start_token_id for model in self.models]\n",
    "        self.decoder_input_ids_list = [torch.tensor([[start_token_id]]).to(\"cuda\") for start_token_id in start_token_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.untranslated_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        untranslated_text = self.untranslated_texts[idx]\n",
    "        concatted_outputs = self.create_model_input(untranslated_text)\n",
    "    \n",
    "        best_model_idx = torch.tensor(int(self.best_model_idx_labels[idx]))\n",
    "        \n",
    "        return concatted_outputs, best_model_idx\n",
    "    \n",
    "    def create_model_input(self, untranslated_text):\n",
    "        with torch.no_grad():\n",
    "            tokenized_texts = [tokenizer(untranslated_text, return_tensors=\"pt\").to(\"cuda\") for tokenizer in self.model_tokenizers]\n",
    "            output_logits = [model(**tokenized_text, decoder_input_ids=decoder_input_ids).logits for model, tokenized_text, decoder_input_ids in zip(models, tokenized_texts, self.decoder_input_ids_list)]\n",
    "            concatted_outputs = torch.cat(output_logits, dim=-1)\n",
    "            concatted_outputs = concatted_outputs.squeeze()\n",
    "        return concatted_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, dataset, batch_size, learning_rate, num_epoch, model_path=None):\n",
    "    \"\"\"\n",
    "    Complete the training procedure below by specifying the loss function\n",
    "    and optimizers with the specified learning rate and specified number of epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for step, data in enumerate(data_loader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            untranslated_text = data[0].to('cuda')\n",
    "            best_model_idx = data[1].to('cuda')\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # do forward propagation\n",
    "            probs = model(untranslated_text)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = criterion(probs, best_model_idx)\n",
    "\n",
    "\n",
    "            # do backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # do the parameter optimization\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate running loss value for non padding\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # print loss value every 100 iterations and reset running loss\n",
    "            if step % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.10f' %\n",
    "                    (epoch + 1, step + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    # define the checkpoint and save it to the model path\n",
    "    # tip: the checkpoint can contain more than just the model\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "    print('Model saved in ', model_path)\n",
    "    print('Training finished in {} minutes.'.format((end - start).seconds / 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Init training data\n",
    "subset_size = 1000\n",
    "dataset = TrainingDataset(\"filtered_train_moe_text.txt\", \"filtered_train_moe_labels.txt\", models, tokenizers)\n",
    "indices = list(range(subset_size))  # Define a list of indices\n",
    "subset = Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 882.2700651270\n",
      "[1,   200] loss: 211.9515381194\n",
      "[1,   300] loss: 176.9301564360\n",
      "[1,   400] loss: 89.5297221100\n",
      "[1,   500] loss: 48.2920158433\n",
      "[2,   100] loss: 62.6752626082\n",
      "[2,   200] loss: 24.5777661010\n",
      "[2,   300] loss: 12.8263265242\n",
      "[2,   400] loss: 61.0250398494\n",
      "[2,   500] loss: 34.1846045462\n",
      "[3,   100] loss: 20.6639028169\n",
      "[3,   200] loss: 35.1202390841\n",
      "[3,   300] loss: 31.6503650487\n",
      "[3,   400] loss: 15.4539192250\n",
      "[3,   500] loss: 21.2763187668\n",
      "Model saved in  model.pt\n",
      "Training finished in 22.333333333333332 minutes.\n"
     ]
    }
   ],
   "source": [
    "train(EnsembleModel().to('cuda'), subset, 2, 0.001, 3, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence_from_model(dataset, model, untranslated_text):\n",
    "    model_input = dataset.create_model_input(untranslated_text)\n",
    "    best_idx = torch.argmax(model(model_input))\n",
    "    model_chosen = dataset.models[best_idx]\n",
    "    model_tokenizer_chosen = dataset.model_tokenizers[best_idx]\n",
    "    \n",
    "    inputs = model_tokenizer_chosen(untranslated_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model_chosen.generate(**inputs)\n",
    "    decoded_outputs = model_tokenizer_chosen.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_46656\\4219589001.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('model.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('model.pt')\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "trained_model = EnsembleModel().to('cuda')\n",
    "trained_model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n"
     ]
    }
   ],
   "source": [
    "with open('wmttest2022.zh','r') as train_moe_labels_file, open('pred.txt', 'w') as filtered_train_moe_labels_file:\n",
    "    for i, best_idx in enumerate(train_moe_labels_file):\n",
    "        pred = predict_sentence_from_model(dataset, EnsembleModel(), best_idx)\n",
    "        filtered_train_moe_labels_file.write(pred + '\\n')\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental stuff just ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65000,   904,   181,    56, 39307,    23,     0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def translate_with_model(model, tokenizer, text, num_beams=5):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, num_beams=num_beams, early_stopping=True)\n",
    "    decoded_outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(inputs['input_ids'].size())\n",
    "    # print(outputs.size())\n",
    "    print(decoded_outputs)\n",
    "    return outputs\n",
    "\n",
    "statement_to_translate = \"这个苹果怎么样\"\n",
    "outputs = translate_with_model(models[0], tokenizers[0], statement_to_translate)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"这些成果的主要研究者都是学生，研究覆盖了环境、机械、能源、医疗、生命科学、人文教育等各大领域，同学们从一个好奇的点子开始，创造出了许多具有应用价值的高端发明，其中一些项目已在国内国际获奖。\"\n",
    "\n",
    "t1, t2 = [translate_with_model(model, tokenizer, example) for model, tokenizer in zip(models, tokenizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token_id1 = models[0].config.decoder_start_token_id\n",
    "decoder_input_ids1 = torch.tensor([[start_token_id1]], device=\"cuda\")\n",
    "\n",
    "inputs1 = tokenizers[0](example, return_tensors=\"pt\").to('cuda')\n",
    "logits1 = models[0](**inputs1, decoder_input_ids=decoder_input_ids1).logits\n",
    "\n",
    "\n",
    "start_token_id2 = models[0].config.decoder_start_token_id\n",
    "decoder_input_ids2 = torch.tensor([[start_token_id2]], device=\"cuda\")\n",
    "\n",
    "inputs2 = tokenizers[0](example, return_tensors=\"pt\").to('cuda')\n",
    "logits2 = models[1](**inputs2, decoder_input_ids=decoder_input_ids2).logits\n",
    "\n",
    "print(logits1)\n",
    "print(logits2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits1.size(), logits2.size())\n",
    "\n",
    "input_size = logits1.shape[-1] + logits2.shape[-1]\n",
    "hidden_size = 128\n",
    "\n",
    "l1 = torch.nn.Linear(input_size, hidden_size).to(\"cuda\")\n",
    "l2 = torch.nn.Linear(hidden_size, 2).to(\"cuda\")\n",
    "\n",
    "catted_logits = torch.cat([logits1, logits2], dim=-1)\n",
    "x = l1(catted_logits)\n",
    "x = l2(x)\n",
    "torch.nn.Softmax(dim=2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentence_from_model(dataset, EnsembleModel(), example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
